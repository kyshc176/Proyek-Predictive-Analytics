# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/166TkVx7XpmsLjmnCk7t8bS3I51fFfBho

<a href="https://colab.research.google.com/github/SarangGami/Bank-Marketing-Effectiveness-Prediction-supervised-learning/blob/main/Bank_Marketing_Effectiveness_Prediction.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# **Project Title : Bank Marketing Effectiveness Prediction**

##**Project Work flow**

- **Importing Neccessary Libraries**

- **Data Wrangling**

```
      ▪ Gathering Data
      ▪ Assessing Data
      ▪ Cleaning Data
```

- **EDA**

```
      ▪ Univariate Analysis
      ▪ Bivariate Analysis
      ▪ Multivariate Analysis

```

- **Features Engineering**

```
      ▪ check Outliers
      ▪ features transformation
      ▪ features selection
```

- **Remove Multicollinearity**

- **pre-processing**

```   
      ▪ preprocessing columns
      ▪ Splitting Dependent and Independent Variables
      ▪ Handling imbalance class
            
```

- **Model implementation and HyperParameter Tuning**

```
      ▪ Train, Test and Split
      ▪ make pipeline using different algorithms

```

- **Final selection of the model**

- **Conclusion**

***
##**Importing Neccessary Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Data manipulation libraries
import pandas as pd
import numpy as np

# Data visualization libraries
import matplotlib.pyplot as plt
# %matplotlib inline
import matplotlib
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go


# setting style and rcparams
sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (7,4)
matplotlib.rcParams['figure.facecolor'] = '#00000000'
plt.rcParams["font.family"] = "Arial"

# for remove Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

# for handling class imbalance
from imblearn.over_sampling import SMOTE

# Preprocessing libraries
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import PowerTransformer

# for model implementation
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
# For build pipeline
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline

# Model selection libraries
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV

# for performance metrics
from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, log_loss, precision_score, f1_score, recall_score, auc

"""***
## **Data Wrangling**

### **`Data Gathering`**
"""

# Loading the dataset

file_path = 'bank_dataset.csv'
bank = pd.read_csv(file_path)

# check the first 5 rows

bank.head()

# check the last 5 rows

bank.tail()

# check the randomly 5 rows

bank.sample(5)

"""### **`Data Accessing`**"""

# finding out how many rows and columns in our dataset

bank.shape

# check information about all columns

bank.info()

"""**Note :-**

- **7** features are numerical and **10** fetaures are categorical.

"""

# check the duplicate rows

bank.duplicated().sum()

"""**Note :-** There are no any rows in the dataset that are exact duplicates of each other."""

# check the null values

bank.isnull().sum()

"""**Note :-** There are no any null values in dataset."""

# check the missing values (NaN values)

bank.isna().sum()

"""**Note:-** There are no any missing values(NaN values) in dataset."""

# describe all the numerical columns

bank.describe(percentiles=[.25,.50,.75])

"""**Note :-**

- Kolom **pdays** berisi nilai **-1**, yang mengindikasikan bahwa nasabah tertentu tidak dihubungi sebelumnya sebagai bagian dari kampanye pemasaran bank.

- Dataset ini mencakup **sejumlah besar nasabah baru**, yang dibuktikan dengan prevalensi nilai 0 pada kolom sebelumnya, yang menunjukkan bahwa banyak nasabah yang belum pernah dihubungi sebelum kampanye ini.

- Kolom saldo berisi beberapa nilai negatif, yang dapat mengindikasikan bahwa beberapa nasabah tertentu telah melakukan penarikan dana secara berlebihan pada rekening mereka atau memiliki hutang yang belum dibayar. Saldo negatif ini menunjukkan bahwa beberapa nasabah mungkin mengalami kesulitan atau tantangan keuangan.

- Tampaknya terdapat sejumlah kecil outlier pada kolom **saldo** dan **durasi**, seperti yang ditunjukkan oleh kesenjangan yang mencolok antara nilai persentil 99.9% dan nilai maksimum pada kolom-kolom ini. Hal ini menunjukkan bahwa mungkin ada beberapa nilai ekstrem yang membuat distribusi data di kolom-kolom ini miring.

"""

# check target columns value counts

bank['y'].value_counts()

"""**Note :-**

- Kolom target sangat tidak seimbang, dengan **rasio yang rendah antara respons positif (ya) dan respons negatif (tidak).** Hal ini diharapkan dalam kampanye pemasaran, karena hanya **sebagian kecil audiens target yang mungkin tertarik dengan produk yang ditawarkan**, seperti deposito berjangka dalam kasus ini. Namun, representasi yang terlalu banyak dari jawaban “tidak” dibandingkan dengan jawaban “ya” menunjukkan adanya ketidakseimbangan kelas, yang harus diperhitungkan ketika membangun model prediktif untuk memastikan hasil yang akurat.

### **`Data Cleaning`**
"""

# create new data frame from original dataset for further data analysis.

bank_df = bank.copy()

# change the datatype of columns

# get the names of all categorical columns
categorical_columns = bank_df.select_dtypes(include='object').columns

# convert all categorical columns to 'category' data type
bank_df[categorical_columns] = bank_df[categorical_columns].astype('category')

# check the data types of all columns
bank_df.info()

# Lets check how many customers balance is 0 or negative

bank_df[(bank_df['balance']<=0)].value_counts().sum()

# Let's check how many customers subscribed to term deposit despite having a balance of 0 or negative

bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes')].value_counts().sum()

"""**Note :-**

- Tampaknya aneh bahwa ada **7280 nasabah dengan saldo bank nol atau negatif** yang juga memiliki pinjaman perumahan atau pinjaman pribadi atau kredit macet. Dari 7280 nasabah ini, 502 nasabah akan berlangganan deposito berjangka.

- terdapat persentase yang sangat rendah (6,8%) dari nasabah dengan saldo bank negatif atau nol yang berlangganan deposito berjangka. Hal ini dapat mengindikasikan bahwa terdapat beberapa kesalahan pada data atau nasabah tersebut bukan merupakan target yang baik untuk kampanye pemasaran.

- Kami perlu menyelidiki lebih lanjut data tersebut untuk menentukan apakah ada pola yang dapat dilihat.
"""

# find out the percentage of customers with a primary education who have subscribed to term deposit and have a non-positive bank balance.

str(round(bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes') & (bank_df['education']=='primary')].value_counts().sum() /
          bank_df[(bank_df['balance']<=0) & (bank_df['education']=='primary')].value_counts().sum()*100,2)) + '%'

# find out the percentage of customers with a secondary education who have subscribed to term deposit and have a non-positive bank balance.

str(round(bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes') & (bank_df['education']=='secondary')].value_counts().sum() /
          bank_df[(bank_df['balance']<=0) & (bank_df['education']=='secondary')].value_counts().sum()*100,2)) + '%'

# find out the percentage of customers with a tertiary education who have subscribed to term deposit and have a non-positive bank balance.

str(round(bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes') & (bank_df['education']=='tertiary')].value_counts().sum() /
          bank_df[(bank_df['balance']<=0) & (bank_df['education']=='tertiary')].value_counts().sum()*100,2)) + '%'

# find out the percentage of customers who student and who have subscribed to term deposit and have a non-positive bank balance.

str(round(bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes') & (bank_df['job']=='student')].value_counts().sum() /
          bank_df[(bank_df['balance']<=0) & (bank_df['job']=='student')].value_counts().sum()*100,2)) + '%'

# find out the percentage of customers who retired and who have subscribed to term deposit and have a non-positive bank balance.

str(round(bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes') & (bank_df['job']=='retired')].value_counts().sum() /
          bank_df[(bank_df['balance']<=0) & (bank_df['job']=='retired')].value_counts().sum()*100,2)) + '%'

"""**Observations:-**

- Di antara nasabah dengan saldo bank yang tidak positif, nasabah dengan pendidikan tersier memiliki persentase tertinggi dalam hal kepemilikan deposito berjangka (9,3%), diikuti oleh pendidikan menengah (6,4%) dan pendidikan dasar (5,2%).

- Nasabah dengan saldo bank yang tidak positif memiliki persentase tertinggi dalam memilih deposito berjangka (18,8%), diikuti oleh nasabah pensiunan (11,8%).

- Dari total 7280 nasabah dengan saldo bank tidak positif, hanya 502 nasabah yang memiliki deposito berjangka, mengindikasikan bahwa mayoritas nasabah dengan saldo bank tidak positif tidak memiliki deposito berjangka.

- Mungkin akan berguna untuk mengumpulkan informasi tambahan mengenai nasabah-nasabah ini, seperti riwayat keuangan mereka dan alasan mereka memiliki saldo bank yang tidak positif, untuk mendapatkan pemahaman yang lebih baik mengenai perilaku mereka dan meningkatkan penargetan kampanye pemasaran di masa depan.

- tetapi tidak ada hubungan yang jelas antara kolom-kolom tersebut. Data tersebut tidak seimbang dan tidak masuk akal. Saldo bank yang negatif atau nol dan baki debet membuat nasabah tidak tertarik untuk membuka deposito berjangka. Oleh karena itu, kami menganggap ini sebagai jenis pencilan.

- Kami memutuskan untuk membuang nilai 502 nasabah dengan saldo bank tidak positif yang berlangganan deposito berjangka untuk menghilangkan pencilan. Tindakan ini diambil berdasarkan real time scinario karena hal ini tidak masuk akal, sehingga akan menghemat waktu dan sumber daya sekaligus menghasilkan prediksi yang lebih akurat.
"""

# Dropping unnecessary rows

bank_df = bank_df.drop(bank_df[(bank_df['balance']<=0) & (bank_df['y']=='yes')].index)

# shape of the dataset after removing unnecessary data

bank_df.shape

"""---
## **EDA and Visualization**

Splitting the data in numerical and categorical columns
"""

# find categorical variables

categorical_columns = [column for column in bank_df.columns if (bank_df[column].dtypes=='category')]
categorical_columns

# find numerical variables

numerical_columns = [column for column in bank_df.columns if (bank_df[column].dtypes!='category')]
numerical_columns

"""### **`Univariate and Bivariate Analysis of catagorical variables`**"""

fig = px.histogram(bank_df, x='y', color='y')
fig.update_layout(xaxis_title='Term deposit subscription', yaxis_title='Total Count of customers')
fig.show()

"""**observations :-**

- Berdasarkan analisis pada kolom target “y” (langganan deposito berjangka), kami menemukan kesenjangan yang signifikan pada rasio nasabah yang berlangganan deposito berjangka dan nasabah yang tidak berlangganan. Rasio nasabah yang tidak berlangganan dan yang berlangganan adalah 88:12, yang mengindikasikan bahwa bank menginvestasikan banyak waktu, sumber daya, dan tenaga kerja tanpa menghasilkan hasil yang diinginkan. Untuk mengatasi masalah ini, kami membangun sebuah model prediktif yang secara efisien dapat mengidentifikasi nasabah potensial yang kemungkinan besar akan berlangganan deposito berjangka, sehingga dapat menghemat waktu dan sumber daya.

- Namun, data yang ada juga tidak seimbang, yang dapat berdampak pada akurasi model prediktif. Kami menerapkan berbagai teknik dalam rekayasa fitur untuk menyeimbangkan dataset dan meningkatkan kinerja model.
"""

# check the job categories wise bank customers total count using univariate analysis

fig = px.histogram(bank_df, x='job', color='job')
fig.update_layout(xaxis_title='Job Categories of Bank Customers',yaxis_title='Total Count of customers')
fig.show()

"""**observations:-**

- Berdasarkan visualisasi kategori pekerjaan nasabah bank, kita dapat melihat bahwa bank cenderung menargetkan nasabah dengan peran pekerjaan seperti manajemen, kerah biru, teknisi, admin, dan jasa. Di sisi lain, nasabah dengan peran pekerjaan seperti wiraswasta, wiraswasta, pengangguran, pelajar, dan pembantu rumah tangga cenderung tidak menjadi target bank.

- Pengamatan ini masuk akal karena peran pekerjaan seperti manajemen, kerah biru, teknisi, admin, dan jasa lebih cenderung memiliki pendapatan tetap dan stabilitas keuangan, yang membuat mereka menjadi kandidat potensial untuk layanan bank seperti pinjaman, kartu kredit, dan produk keuangan lainnya. Di sisi lain, pekerjaan seperti wiraswasta, pengangguran, dan pelajar cenderung tidak memiliki penghasilan tetap, yang membuat mereka cenderung tidak ditargetkan oleh bank.
"""

# check the job categories wise customers who subscribe term deposit or not using bivariate analysis

plt.figure(figsize=(18,6))
sns.countplot(x='job',hue='y', data=bank_df, order=bank_df['job'].value_counts().index)
plt.xlabel('Job Categories of Bank Customers', color='black')
plt.ylabel('Total Count of customers', color='black')
plt.tight_layout()
plt.show()

"""**observations :-**
- Histogram kategori pekerjaan nasabah bank menunjukkan bahwa bank menargetkan lebih banyak nasabah dengan kategori pekerjaan seperti manajemen, kerah biru, teknisi, admin, dan jasa, sementara lebih sedikit nasabah dengan kategori pekerjaan seperti wiraswasta, wiraswasta, pengangguran, pelajar, dan pembantu rumah tangga. Namun, menarik untuk dicatat bahwa nasabah dengan kategori pekerjaan pelajar dan pensiunan menunjukkan rasio yang lebih tinggi untuk berlangganan deposito berjangka dibandingkan dengan kategori pekerjaan lainnya.

- Bank harus lebih fokus pada kategori pekerjaan seperti pelajar dan pensiunan untuk meningkatkan tingkat langganan.

"""

# check the Subscription Status per month using bivariate analysis

plt.figure(figsize=(18,6))
sns.countplot(x='month',hue='y', data=bank_df, order=bank_df['month'].value_counts().index)
plt.xlabel('Months', color='black')
plt.ylabel('Subscription Status', color='black')
plt.tight_layout()
plt.show()

"""**observations :-**
- kita dapat mengamati bahwa tingkat berlangganan rendah selama bulan September, Oktober, Desember, Januari, dan Maret, dan jumlah kontak pelanggan juga rendah selama bulan-bulan ini. Namun, tingkat berlangganan relatif baik dibandingkan dengan jumlah kontak pelanggan selama bulan-bulan tersebut. Oleh karena itu, untuk meningkatkan tingkat langganan, kami dapat fokus untuk meningkatkan jumlah kontak nasabah pada bulan-bulan tersebut dan mendorong mereka untuk berlangganan deposito berjangka.

- Jika dibandingkan dengan tingkat langganan pada bulan-bulan lainnya, terlihat bahwa tingkat langganan secara keseluruhan rendah, terlepas dari jumlah kontak yang dilakukan. Hal ini mengindikasikan bahwa bank perlu meningkatkan strategi pemasaran dan promosi deposito berjangka untuk menarik lebih banyak nasabah.
"""

# find percentage of customers in remaining categorical variables using univariate analysis

categorical = ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']

for column in categorical:
    fig = px.pie(bank_df, names=column, title=f"Customers Percentage for column {column}",
                 hole=0.3, color_discrete_sequence=px.colors.qualitative.Plotly)
    fig.update_traces(textinfo='percent+label')
    fig.show()

# bivariate analysis of remaining categorical variables and target column y(term deposit)

categorical = ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']

plt.figure(figsize=(15,15))
n = 1
for column in categorical:
  ax = plt.subplot(4,2,n)
  sns.countplot(x = column, hue='y', data=bank_df, order=bank_df[column].value_counts().index)
  plt.xlabel(column)
  plt.tight_layout()
  n += 1
plt.show()

"""**observations :-**
- Nasabah yang tidak memiliki pinjaman pribadi, pinjaman perumahan, atau kredit macet memiliki rasio yang lebih tinggi untuk berlangganan deposito berjangka dibandingkan dengan nasabah yang memiliki pinjaman atau kredit macet.

- Pada kolom pendidikan, persentase nasabah yang memiliki deposito berjangka lebih tinggi pada nasabah dengan kategori pendidikan menengah dan tinggi. Hal ini menunjukkan bahwa bank harus lebih fokus pada nasabah dengan kategori tersebut.

- Pada kolom pernikahan, tidak ada pola yang jelas dalam rasio langganan. Namun, bank harus lebih fokus pada nasabah yang sudah menikah dan lajang yang masih berstatus pelajar. Hal ini karena kami mengamati pada grafik sebelumnya bahwa pelajar memiliki rasio langganan yang lebih tinggi pada kolom pekerjaan.

- Pada kolom poutcome, kita dapat melihat dengan jelas bahwa nasabah yang berlangganan deposito berjangka pada kampanye pemasaran sebelumnya memiliki kemungkinan lebih besar untuk berlangganan kembali. Hal ini mengindikasikan peluang keberhasilan yang tinggi dalam menargetkan nasabah ini.

### **`Univariate and Bivariate Analysis of numerical variables`**
"""

# check distribution of all numerical column

plt.figure(figsize=(20,20))

n=1
for col in numerical_columns:
  ax = plt.subplot(4,2,n)
  sns.histplot(bank_df[col], kde=True, color ='red')
  n += 1
plt.show()

"""**observations :-**
- Kolom numerik 'usia', 'saldo', dan 'durasi' miring ke kanan, menunjukkan bahwa mungkin ada beberapa outlier yang ada di data. mereka jauh dari kisaran nilai normal dan dapat secara signifikan mempengaruhi analisis

- Sesuai histogram, ada beberapa nasabah yang berusia di atas 65 tahun dan telah dihubungi untuk deposito berjangka. Hal ini mungkin bukan strategi yang masuk akal karena nasabah ini mungkin tidak memiliki jangka waktu investasi yang cukup panjang untuk mendapatkan keuntungan dari deposito berjangka. kita harus menganalisis rasio nasabah usia tua yang berlangganan dan yang tidak berlangganan.

"""

# Visualizing the distribution of customers subscribed or not term deposit using a violin plot with numerical columns

columns = ['age','balance','duration','campaign']

plt.figure(figsize=(18,12))
n=1
for col in columns:
  ax = plt.subplot(2,2,n)
  sns.violinplot(x='y', y=col ,data=bank_df)
  plt.xlabel('Term Deposit')
  n += 1
plt.show()

"""**observations:-**
- `Usia:` Dari plot biola, kita dapat mengamati bahwa nasabah dalam **kelompok usia 20 hingga 30** memiliki peluang lebih tinggi untuk berlangganan deposito berjangka. Namun, setelah usia **60+**, rasio nasabah yang berlangganan juga tinggi. Penting untuk dicatat bahwa jumlah nasabah yang dihubungi pada kelompok usia **60+** lebih sedikit, yang dapat menjadi faktor dalam tren yang diamati.

- `Saldo :` Distribusi saldo menunjukkan bahwa nasabah dengan **saldo yang lebih kecil cenderung tidak berlangganan deposito berjangka.

- `Durasi :` Kita dapat melihat bahwa ketika durasi kontak terakhir yang dilakukan dengan nasabah lebih tinggi, ada kemungkinan besar nasabah tersebut berlangganan deposito berjangka.

- `Kampanye :` Dari plot biola, kita dapat mengamati bahwa nasabah yang dihubungi **5 kali atau kurang** selama kampanye pemasaran saat ini memiliki kemungkinan lebih besar untuk berlangganan deposito berjangka.

Pengamatan ini menunjukkan bahwa usia, saldo, durasi, dan kampanye merupakan faktor penting dalam menentukan apakah nasabah akan berlangganan deposito berjangka.

### **`Bivariate Analysis of both categorical and numerical variables`**
"""

bank_df.head()

plt.figure(figsize=(18,6))

sns.countplot(x='job', hue='default', data=bank_df)
plt.xlabel('Job Categories of Bank Customers', color='black')
plt.ylabel('Total Count of Credit Default', color='black')

plt.tight_layout()
plt.show()

"""**observations :-**
- Berdasarkan analisis sebelumnya, dapat dilihat bahwa nasabah pelajar dan pensiunan memiliki kemungkinan yang lebih tinggi untuk memilih deposito berjangka. Hal ini dapat disebabkan oleh fakta bahwa mereka memiliki jumlah kredit macet yang lebih rendah, serta berpotensi memiliki lebih banyak waktu dan sumber daya yang tersedia untuk mempertimbangkan investasi jangka panjang.

- Selain itu, penting bagi bank untuk fokus pada nasabah dengan kategori pekerjaan tertentu, seperti pekerja kantoran, admin, manajemen, dan jasa. Kategori-kategori ini telah menunjukkan porsi yang signifikan dari nasabah yang berlangganan deposito berjangka.

- Di sisi lain, mungkin bukan strategi yang baik untuk menghubungi nasabah dalam kategori seperti pengusaha, pengangguran, dan wiraswasta karena mereka telah menunjukkan kemungkinan yang lebih rendah untuk berlangganan deposito berjangka. Namun, penting untuk dicatat bahwa mungkin ada faktor lain seperti saldo bank dan status pinjaman yang juga dapat mempengaruhi keputusan nasabah untuk berlangganan.
"""

# check the average duration of customers based on term deposit

plt.figure(figsize=(12,6))

sns.barplot(y='duration',x='y',data=bank_df)
plt.tight_layout()
plt.show()

"""**observation :-**  Jika durasi kontak terakhir yang dilakukan dengan pelanggan adalah **500+ detik**, maka rasio pelanggan yang berlangganan dan yang tidak berlangganan menjadi sama dan lebih berpihak pada pelanggan yang berlangganan."""

# find out which day of the month was the client contacted more

types = bank_df.groupby("day")['y'].value_counts(normalize=False).unstack()
types.plot(kind='bar', stacked='True', xlabel=('Day of the month'), ylabel=('Term Deposit'), figsize=(15,6))
plt.tight_layout()
plt.show()

"""- Jumlah nasabah yang lebih besar dihubungi pada pertengahan bulan. Namun demikian, rasio deposito berjangka tetap relatif konsisten sepanjang hari dalam satu bulan, kecuali pada hari terakhir (hari ke-30), yang memiliki peluang sedikit lebih tinggi untuk nasabah yang berlangganan deposito berjangka. Oleh karena itu, akan bermanfaat bagi bank untuk lebih fokus pada hari terakhir dalam satu bulan untuk menghubungi nasabah. Hal ini dapat disebabkan oleh berbagai alasan, seperti nasabah lebih bersedia untuk berinvestasi pada akhir bulan ketika mereka memiliki pemahaman yang lebih baik tentang anggaran bulanan mereka, atau bank memiliki penawaran atau insentif yang lebih baik pada akhir bulan untuk mendorong nasabah untuk berinvestasi pada deposito berjangka."""

# checking the corr. of balance and duration column

plt.figure(figsize=(12,6))

sns.scatterplot(x='balance', y='duration', data=bank_df)
plt.tight_layout()
plt.show()

"""**Note:-**
- Dari scatter plot, kita dapat melihat bahwa nasabah dengan saldo bank rendah atau nol lebih sering dihubungi oleh bank. Strategi ini mungkin bukan strategi yang paling efektif karena nasabah dengan saldo rendah memiliki rasio deposito berjangka yang lebih tinggi. Oleh karena itu, bank harus fokus pada nasabah dengan saldo menengah dan tinggi ketika menghubungi mereka untuk meningkatkan kemungkinan hasil yang positif.

---
## **Features Engineering**

### **`Check Outliers`**
"""

# ploting a boxplot for numerical features to check the outliers

columns = ['age','balance']

plt.figure(figsize=(22,5))

n = 1
for col in columns:
    ax = plt.subplot(1,2,n)
    sns.boxplot(x=bank_df[col], color='#8FBC8F')
    plt.xlabel(col)
    n += 1
plt.show()

"""- Dalam skenario ini, tidak disarankan untuk menghapus pencilan dari dataset karena mengandung informasi yang berharga. Sebagai contoh, pada kolom saldo bank, beberapa nasabah memiliki saldo yang tinggi sementara beberapa lainnya memiliki saldo yang tidak positif. Jika kita menghapus pencilan ini, model kita tidak akan memiliki informasi yang cukup untuk membuat prediksi yang akurat, dan ada risiko overfitting.

- Demikian pula, untuk kolom usia, terdapat pencilan dalam dataset, tetapi kita perlu melatih model kita untuk membuat prediksi berdasarkan semua faktor yang tersedia. Jika kita menghapus pencilan, kita mungkin akan kehilangan informasi penting yang dapat mempengaruhi akurasi model kita. Oleh karena itu, penting untuk menjaga outlier dalam dataset dan menggunakan teknik yang tepat untuk menanganinya selama pelatihan model. di sini tidak ada masalah karena kami menggunakan algoritma yang kuat.

### **`features transformation and selection`**
"""

# converting job column into new categorical column by assinging categories

def job_category(job):
  cat_1 = ['retired','student']
  cat_2 = ['blue-collar','management','technician','admin.','services']
  cat_3 = ['unemployed','housemaid','unknown']
  cat_4 = ['self-employed','entrepreneur']

  if job in cat_1 :
      return 'cat1'
  if job in cat_2 :
      return 'cat2'
  if job in cat_3 :
      return 'cat3'
  if job in cat_4 :
      return 'cat4'
  return job

# apply function job_category for add new column in dataset

bank_df['job_categories']=bank_df['job'].apply(job_category)

bank_df.sample()

# explore new column with term deposit(y) column

types = bank_df.groupby('job_categories')['y'].value_counts(normalize=False).unstack()
types.plot(kind='bar', xlabel=('Job Categories'), ylabel=('Term Deposit'), figsize=(15,6))
plt.tight_layout()
plt.show()

"""**observations :-**
- kami mengamati bahwa kategori 1 dan 2 memiliki rasio langganan deposito berjangka yang tinggi. Namun, kami melihat bahwa bank lebih banyak menghubungi nasabah kategori 2 dibandingkan dengan kategori 1. Hal ini menunjukkan bahwa bank harus lebih fokus pada nasabah kategori 2, tetapi pada saat yang sama, bank juga tidak boleh mengabaikan nasabah kategori 1, karena meskipun jumlah nasabahnya lebih sedikit, namun rasio nasabah yang berlangganan masih tinggi.

- Bank harus mencoba untuk mencapai keseimbangan antara kedua kategori ini dan mengalokasikan sumber daya mereka secara tepat untuk memaksimalkan tingkat berlangganan mereka.
"""

# converting age column into new categorical column by assinging categories

def age_category(age):
  if age < 30:
     return 'struggling'
  elif age < 50:
     return 'stable'
  elif age < 60:
     return 'about to retire'
  elif age < 75:
     return 'old age'
  else:
     return 'counting a last breathe'

# apply function age_category for add new column in dataset

bank_df['age_categories']=bank_df['age'].apply(age_category)

bank_df.sample()

# explore new column age_categories with term deposit(y) column

plt.figure(figsize=(18,6))

sns.countplot(x='age_categories', hue='y', data=bank_df)
plt.xlabel('age_categories', color='black')
plt.tight_layout()
plt.show()

"""- bank harus lebih fokus pada kategori yang sedang berjuang dan akan pensiun juga"""

# scatterplot

plt.figure(figsize=(15,8))
sns.scatterplot(data=bank_df, x='age_categories',y='duration',hue='y')
plt.tight_layout()
plt.show()

"""**observations:-**

- Kesimpulan Dari scatterplot di atas dapat disimpulkan bahwa ketika kategori usia old_age dan stabil maka durasi komunikasi lebih banyak dan ketika durasi tinggi maka kemungkinan besar klien akan berlangganan deposito berjangka.

- Dari scatterplot di atas kita dapat menyimpulkan bahwa ketika durasi kontak dari 300 hingga 2000 di kolom stabil, berjuang, akan pensiun dan saldo klien berada di kisaran tengah 500-35000 maka klien tersebut sebagian besar berlangganan deposito berjangka.
"""

# Rename target column 'y' to 'term_deposit'

bank_df = bank_df.rename(columns={'y':'term_deposit'})

# converting those features into binary class numeric features

bank_df["default"]=bank_df["default"].apply(lambda x : 1 if x=='yes' else 0)
bank_df["loan"]=bank_df["loan"].apply(lambda x : 1 if x=='yes' else 0)
bank_df["housing"]=bank_df["housing"].apply(lambda x : 1 if x=='yes' else 0)
bank_df["term_deposit"]=bank_df["term_deposit"].apply(lambda x : 1 if x=='yes' else 0)

bank_df.sample()

# droppping columns because we have extracted new features from that columns.

bank_df.drop(columns=['age','job','pdays','duration'], axis=1, inplace=True)

"""**Note :-**

- Kolom 'pdays' dan 'duration' telah dihapus dari dataset karena tidak relevan untuk membangun model prediktif untuk pelanggan baru. 'duration' mewakili durasi kontak terakhir dengan pelanggan di kampanye sebelumnya, dan 'pdays' mewakili jumlah hari yang telah berlalu sejak pelanggan terakhir kali dihubungi. Karena kami sedang membangun model untuk pelanggan baru, informasi ini tidak tersedia untuk mereka. Oleh karena itu, menghapus kolom ini akan mencegah bias atau overfitting yang mungkin terjadi akibat penggunaan data yang tidak relevan atau tidak tersedia.
"""

plt.figure(figsize=(16, 8))
sns.heatmap(bank_df.select_dtypes(include=[float, int]).corr().abs(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix (Numeric Features Only)')
plt.show()

"""- tidak ada multikolinearitas yang signifikan antara variabel-variabel independen. Oleh karena itu, kita dapat menyimpulkan bahwa model tersebut robust dan variabel independen cocok untuk memprediksi variabel dependen.

---
## **pre-processing**

### **`Encoding categorical columns`**
"""

# Creating dummy variables for categorical variables

marital = pd.get_dummies(bank_df['marital'],prefix='marital')
contact = pd.get_dummies(bank_df['contact'], prefix='contact')
poutcome = pd.get_dummies(bank_df['poutcome'], prefix = 'poutcome')
month = pd.get_dummies(bank_df['month'], prefix = 'month')
age_cat = pd.get_dummies(bank_df['education'],prefix = 'education')
job = pd.get_dummies(bank_df['job_categories'],prefix = 'job_categories')
education = pd.get_dummies(bank_df['age_categories'],prefix = 'age_categories')

bank_df = pd.concat([bank_df,marital,contact,poutcome,month,education,job,age_cat],axis=1)

# dropping original column

bank_df.drop(columns=['marital','contact','poutcome','month','education','job_categories','age_categories'], axis=1, inplace=True)

# dropping one of the resultant columns

bank_df.drop(columns=['marital_divorced','contact_unknown','poutcome_other','month_dec','education_unknown','job_categories_cat3',
                      'age_categories_counting a last breathe'], axis=1, inplace=True)

bank_df.sample()

bank_df.shape

"""### **`Splitting Dependent and Independent Variables`**"""

#seprating our dependent and independent features

x=bank_df.drop(columns=['term_deposit'],axis=1)
y=(bank_df['term_deposit'])

x.head()

y.head()

# shape of the x and y

print(x.shape)
print(y.shape)

"""### **`SMOTE Oversampling for handling class imbalance`**"""

sampler=SMOTE()
X ,y = sampler.fit_resample(x,y)

# Original length and Resampled Length

print('Original Dataset length',len(x))
print('Resampled Dataset length',len(X))

"""---
## **Model implementation and HyperParameter Tuning**

### **`Train, Test and Split`**
"""

# calling train_test_split() to get the training and testing data.

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 0)

# split sizes
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""### **`All Define functions here for model training`**"""

# Lets define a function for Evaluation metrics and heatmap of confusion metrix so that we can reuse it again and again

def performance_metrics(y_test, prediction, y_train, train_predicted, model=''):
    print('\033[1m' + '-----------------------------------------' + '\033[0m')
    print(f'{model} Test data accuracy Score', accuracy_score(y_test, prediction))
    print(f'{model} Train data accuracy Score', accuracy_score(y_train, train_predicted))
    print('\033[1m' + '-----------------------------------------' + '\033[0m')
    print(classification_report(y_test, prediction))
    print('\033[1m' + '-----------------------------------------' + '\033[0m')

    conf_matrix = confusion_matrix(y_test, prediction)
    print(conf_matrix)

    plt.figure(figsize=(8, 6))
    ax = sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')
    ax.set_title(f'Confusion Matrix for {model}')
    ax.set_xlabel('Predicted Values')
    ax.set_ylabel('Actual Values')
    ax.set_xticklabels(['No', 'Yes'])
    ax.set_yticklabels(['No', 'Yes'])

    return plt.show()

"""- Algoritma seperti KNN, Naive bayes, Regresi Logistik, dan SVM membutuhkan data yang terstandarisasi

"""

# so stored standardize and normalize data in step1 and step0 using StandardScaler and MinMaxScaler through column transformer

step1 = ColumnTransformer(transformers=[
    ('col_tnf', StandardScaler(),list(range(35))),
],remainder='passthrough')

step0 = ColumnTransformer(transformers=[
     ('col_tnf', MinMaxScaler(),list(range(35))),
],remainder='passthrough')

"""## **`K_nearest Neighbour Classifier(KNN) with hyperparameter tuning`**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer


# giving parameters different odd values of K(n_neighbors) to find maximum Recall score for Yes(1).
parameters = {'n_neighbors':np.arange(5,20),
              'weights':['uniform']
}

# use gridsearchCV
knn = GridSearchCV(KNeighborsClassifier(), param_grid=parameters , cv=10, n_jobs=-1)

step2 = knn


# Add an imputer step to handle missing values
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median' or 'most_frequent'

# make pipeline
pipe1 = Pipeline([
    ('imputer', imputer),  # Add the imputer step
    ('step1', step1),
    ('step2', step2)
])

# fit the pipeline on training dataset
pipe1.fit(X_train, y_train)

# predict the test dataset
y_pred_train_knn = pipe1.predict(X_train)
y_pred_knn = pipe1.predict(X_test)

print('\n')
print('\033[1mCross-validation score and best params\033[0m')
print("The best parameters is", knn.best_params_)
print('cross-validation score', knn.best_score_)

# Evaluate performance
performance_metrics(y_test, y_pred_knn, y_train, y_pred_train_knn, 'KNN')

"""**Note :-**
- kami lebih tertarik untuk memprediksi dengan benar nasabah yang menjawab Ya untuk deposito berjangka. proporsi positif aktual (Ya) yang diklasifikasikan dengan benar oleh model. Dalam kasus kami, skor recall yang tinggi untuk Ya berarti bahwa kami mengidentifikasi dengan benar nasabah yang lebih mungkin untuk berlangganan deposito berjangka. jadi, kami perlu fokus pada skor recall Ya (1) dan skor F1 untuk metrik evaluasi kami.

- Skor Recall dan skor F1 yang diperoleh untuk model pengklasifikasi KNN masing-masing adalah 0.91 dan 0.93. dan skor presisi tinggi 0.95. kita masih harus mengurangi nilai negatif palsu yang diprediksi (748) ...

- Terdapat perbedaan yang sangat kecil antara akurasi training dan testing sehingga tidak terjadi overfitting atau underfitting. dan nilai cross validation juga sangat baik.

- Secara keseluruhan, nilai ini menunjukkan bahwa KNN adalah titik awal yang baik untuk implementasi model kami. Namun, kita perlu mengevaluasi lebih banyak algoritma model untuk memilih yang terbaik untuk masalah bisnis spesifik kita.

## **`Support vector machines (SVM)`**
"""

from sklearn.svm import SVC
from sklearn.impute import SimpleImputer

svc = SVC()
step2 = svc

# make pipeline
pipe2 = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Add imputer to handle NaN values
    ('step1', step1),
    ('step2', step2)
])

# fit the pipeline on training dataset
pipe2.fit(X_train, y_train)

# predict the train and test dataset
y_pred_train_svc = pipe2.predict(X_train)  # Fixed pipe4 to pipe2
y_pred_svc = pipe2.predict(X_test)  # Fixed pipe4 to pipe2

# check the accuracy_score, classification_report and heatmap of confusion_matrix
performance_metrics(y_test, y_pred_svc, y_train, y_pred_train_svc, 'SVM Classifier')

"""**Notes :-**
- Berdasarkan metrik evaluasi seperti skor recall dan F1-score, terlihat bahwa algoritma SVM memiliki performa yang lebih baik daripada algoritma Naive Bayes dan Regresi Logistik. Nilai recall dan F1-score untuk SVM masing-masing adalah 0.90 dan 0.93, yang lebih tinggi daripada nilai yang sesuai untuk Naive Bayes dan Regresi Logistik.

- Namun, mungkin ada baiknya mencoba algoritme lain seperti Random Forest Classifier dan xgboost classifier untuk melihat apakah ada ruang untuk peningkatan lebih lanjut dalam hal kinerja.

## **`Random Forest Classifier with hyperparameter tuning`**
"""

from sklearn.ensemble import RandomForestClassifier

# giving parameters
parameters = {
    'n_estimators':[100,300,500,800,1000],
    'max_depth' :[6,8,10,13,15,18,20]
}

# we use gridsearchCV because the dataset is not that big so use this not RandomizedSearchCV
rfc = GridSearchCV(RandomForestClassifier(), param_grid=parameters, cv=10)

step2 = rfc

# make pipeline
pipe3 = Pipeline([
    ('step1',step1),
    ('step2',step2)
])

# fit the pipeline on training dataset
pipe3.fit(X_train,y_train)

# predict the train and test dataset
y_pred_train_rfc = pipe3.predict(X_train)
y_pred_rfc = pipe3.predict(X_test)

print('\n')
print('\033[1mCross-validation Test score and best params\033[0m')
print("The best parameters is", rfc.best_params_)
print('cross-validation score', rfc.best_score_)

# check the accuracy_score, classification_report and heatmap of confusion_matrix
performance_metrics(y_test,y_pred_rfc,y_train,y_pred_train_rfc,'RandomForestClassifier')

"""- recall dan F1-score dari pengklasifikasi random forest adalah 0.92 dan 0.94, yang mana 1% lebih tinggi dari KNN. Ukuran dataset uji coba sekitar 18 ribu dan skor recall 1% lebih baik dari KNN (model terbaik sebelumnya), hal ini menunjukkan bahwa pengklasifikasi hutan acak adalah algoritma yang menjanjikan untuk masalah ini.

- Nilai validasi silang sebesar 0.938 menunjukkan bahwa model ini dapat menggeneralisasi dengan baik pada data baru. Nilai akurasi pada data uji adalah 0.94 dan pada data latih adalah 0.97, yang mengindikasikan bahwa tidak ada overfitting atau underfitting.

## **Plot ROC and compare AUC**
"""

from sklearn.metrics import roc_curve, auc

knn_fpr, knn_tpr, threshold = roc_curve(y_test, y_pred_knn)
auc_knn = auc(knn_fpr, knn_tpr)
svc_fpr, svc_tpr, threshold = roc_curve(y_test, y_pred_svc)
auc_svc = auc(svc_fpr, svc_tpr)
rfc_fpr, rfc_tpr, threshold = roc_curve(y_test, y_pred_rfc)
auc_rfc = auc(rfc_fpr, rfc_tpr)

classifiers = {
    'KNN': (y_pred_knn, knn_fpr, knn_tpr),
    'SVM': (y_pred_svc, svc_fpr, svc_tpr),
    'RandomForest Classifier': (y_pred_rfc, rfc_fpr, rfc_tpr)
}

plt.figure(figsize=(7,7), dpi=100)

for clf_name, (y_pred, fpr, tpr) in classifiers.items():
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, marker='.', label='%s (auc = %0.3f)' % (clf_name, auc_score))

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')
plt.tight_layout()
plt.legend()
plt.show()

"""---
## **Final Summary of Conclusion**

### **`Conclusion`**

- Setelah mengevaluasi kinerja berbagai algoritma machine learning pada dataset yang diberikan, ditemukan bahwa Random Forest Classifier mengungguli algoritma lain seperti KNN, SVM, Naive Bayes, dan Regresi Logistik. Hyperparameter dari Random Forest Classifier disetel menggunakan GridSearchCV untuk menemukan kombinasi parameter terbaik untuk kinerja yang optimal.

- Skor validasi silang untuk Random Forest Classifier ditemukan sebesar 0,937, yang menunjukkan bahwa model menggeneralisasi dengan baik untuk data yang tidak terlihat. Skor akurasi untuk data uji adalah 0,94, yang merupakan indikasi yang baik dari kemampuan model untuk berkinerja baik pada data baru yang tidak terlihat. Selain itu, nilai akurasi untuk data latih adalah 0,97, yang mengindikasikan bahwa model tidak terlalu cocok dengan data latih.

- Selain itu, Random Forest Classifier mencapai skor recall 0,92 dan skor F1 0,94, yang sedikit lebih baik daripada skor yang sesuai untuk KNN dan SVM. Oleh karena itu, berdasarkan metrik evaluasi dan hasil kinerja, dapat disimpulkan bahwa **Random Forest Classifier** adalah algoritma yang cocok untuk masalah ini.

- Dengan menggunakan hasil ini, bank dapat secara khusus menargetkan klien dan mendapatkan kesuksesan yang lebih tinggi dalam usaha mereka. Menghemat banyak waktu dengan tidak berfokus pada nasabah yang memiliki probabilitas lebih rendah adalah keuntungan lain dari proyek ini.
"""